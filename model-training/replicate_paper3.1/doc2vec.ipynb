{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from ipynb.fs.defs.helper import report_to_csv, export_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data source\n",
    "models = [ \n",
    "            GaussianNB(),\n",
    "            BernoulliNB(),\n",
    "            LinearSVC(), \n",
    "            LogisticRegression(solver='liblinear', random_state=42, max_iter=100000),\n",
    "            DecisionTreeClassifier(),\n",
    "            KNeighborsClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "#             xgboost.XGBClassifier(),\n",
    "            ExtraTreesClassifier(n_estimators=200),\n",
    "            VotingClassifier(estimators=[\n",
    "                ('lr', LogisticRegression(solver='liblinear', random_state=42, max_iter=100000)), \n",
    "                ('dt', DecisionTreeClassifier()), \n",
    "                ('lsvc', LinearSVC()), \n",
    "                ('knn', KNeighborsClassifier()), \n",
    "                ('nb', GaussianNB()), \n",
    "                ('bnb', BernoulliNB()), \n",
    "            ], voting='hard'),\n",
    "            AdaBoostClassifier(random_state=42), \n",
    "            GradientBoostingClassifier(learning_rate=0.01, random_state=42),\n",
    "        ]\n",
    "\n",
    "text_features = [\n",
    "                'comment', \n",
    "                'lemmatized_comment', \n",
    "                'stopwords_removal',\n",
    "                'stopwords_removal_lemmatization',\n",
    "                ]\n",
    "\n",
    "user_reviews = {}\n",
    "\n",
    "user_reviews['bug'] = {'data_train': 'Bug_Report_Data_Train.json', \n",
    "                      'not_data_train': 'Not_Bug_Report_Data_Train.json',\n",
    "                      'data_test': 'Bug_Report_Data_Test.json',\n",
    "                      'not_data_test': 'Not_Bug_Report_Data_Test.json',\n",
    "                      'label': 'Bug',\n",
    "                      'not_label': 'Not Bug'}\n",
    "\n",
    "user_reviews['feature'] = {'data_train': 'Feature_OR_Improvment_Request_Data_Train.json', \n",
    "                          'not_data_train': 'Not_Feature_OR_Improvment_Request_Data_Train.json',\n",
    "                          'data_test': 'Feature_OR_Improvment_Request_Data_Test.json',\n",
    "                          'not_data_test': 'Not_Feature_OR_Improvment_Request_Data_Test.json',\n",
    "                          'label': 'Feature',\n",
    "                          'not_label': 'Not Feature'}\n",
    "\n",
    "user_reviews['ux'] = {'data_train': 'UserExperience_Data_Train.json', \n",
    "                        'not_data_train': 'Not_UserExperience_Data_Train.json',\n",
    "                        'data_test': 'UserExperience_Data_Test.json',\n",
    "                        'not_data_test': 'Not_UserExperience_Data_Test.json',\n",
    "                        'label': 'UserExperience',\n",
    "                        'not_label': 'Not UserExperience'}\n",
    "\n",
    "user_reviews['rating'] = {'data_train': 'Rating_Data_Train.json', \n",
    "                          'not_data_train': 'Not_Rating_Data_Train.json',\n",
    "                          'data_test': 'Rating_Data_Test.json',\n",
    "                          'not_data_test': 'Not_Rating_Data_Test.json',\n",
    "                         'label': 'Rating',\n",
    "                         'not_label': 'Not Rating'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_review_type = user_reviews['bug'] # bug, feature, ux, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('../RE2015_data/json_data/' + selected_review_type['data_train']) as data_file:    \n",
    "    data_train = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['not_data_train']) as data_file:    \n",
    "    not_data_train = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['data_test']) as data_file:    \n",
    "    data_test = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['not_data_test']) as data_file:    \n",
    "    not_data_test = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data frame\n",
    "data_train = pd.DataFrame.from_dict(data_train, orient='columns')\n",
    "data_train['label'] = selected_review_type['label']\n",
    "\n",
    "data_test = pd.DataFrame.from_dict(data_test, orient='columns')\n",
    "data_test['label'] = selected_review_type['label']\n",
    "\n",
    "not_data_train = pd.DataFrame.from_dict(not_data_train, orient='columns')\n",
    "not_data_train['label'] = selected_review_type['not_label']\n",
    "\n",
    "not_data_test = pd.DataFrame.from_dict(not_data_test, orient='columns')\n",
    "not_data_test['label'] = selected_review_type['not_label']\n",
    "\n",
    "df_train = data_train.append(not_data_train, ignore_index=True)\n",
    "df_test = data_test.append(not_data_test, ignore_index=True)\n",
    "\n",
    "df = df_train.append(df_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comment1 = \"This app serves its purpose for me perfectly except for the mobile deposit won't work. It keeps saying can't find endorsement. After calling PNC multiple times about this still no fix.\"\n",
    "# stopwords_removal1 = \"this app serves purpose for perfectly except for mobile deposit wont work keeps saying cant find endorsement after calling pnc multiple times about this still no fix\"\n",
    "# lemmatized_comment1 = \"this app serve it purpose for me perfectly except for the mobile deposit wont work it keep say cant find endorsement after call pnc multiple time about this still no fix\"\n",
    "# stopwords_removal_lemmatization1 = \"this app serve purpose for perfectly except for mobile deposit wont work keep say cant find endorsement after call pnc multiple time about this still no fix\"\n",
    "\n",
    "# train = label_sentences([stopwords_removal_lemmatization1], 'Train')\n",
    "\n",
    "# model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "# model_dbow.build_vocab([x for x in tqdm(train)])\n",
    "    \n",
    "# for epoch in range(30):\n",
    "#     model_dbow.train(utils.shuffle([x for x in tqdm(train)]), total_examples=len(train), epochs=1)\n",
    "#     model_dbow.alpha -= 0.002\n",
    "#     model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "# train_vectors_dbow = get_vectors(model_dbow, len(train), 300, 'Train')\n",
    "\n",
    "# print(train_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rank = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for text in text_features:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[text], df.label, random_state=42, test_size=0.3)\n",
    "    X_train = label_sentences(X_train, 'Train')\n",
    "    X_test = label_sentences(X_test, 'Test')\n",
    "    \n",
    "    all_data = X_train + X_test\n",
    "    \n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "    model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "        \n",
    "    train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "    test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n",
    "    \n",
    "    for model in models:\n",
    "        print(model.__class__.__name__)\n",
    "        print(text)\n",
    "        \n",
    "        model.fit(train_vectors_dbow, y_train)\n",
    "        y_pred = model.predict(test_vectors_dbow)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('accuracy %s\\n' % accuracy)\n",
    "        \n",
    "        description = '%s/doc2vec + %s + %s' % (selected_review_type['label'], text, model.__class__.__name__)\n",
    "        accuracy_rank.append((accuracy, description))\n",
    "        \n",
    "        # Export model\n",
    "        export_model(model, file_name=description)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True) \n",
    "        report['accuracy'] = {' ': accuracy}\n",
    "        report_to_csv(report, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sorted results to text file.\n",
    "text_file = open('results/Rank/' + selected_review_type['label'] + '_doc2vec_rank_output.txt', 'w')\n",
    "sorted_accuracy_rank = sorted(accuracy_rank, key=lambda accuracy_rank: accuracy_rank[0], reverse=True)\n",
    "for item in sorted_accuracy_rank:\n",
    "    text = '%f , %s' % (item[0], item[1]) \n",
    "    print(text)\n",
    "    text_file.write(text + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
