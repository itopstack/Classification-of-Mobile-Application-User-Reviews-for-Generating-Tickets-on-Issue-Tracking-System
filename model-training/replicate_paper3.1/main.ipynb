{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pprint\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from ipynb.fs.defs.helper import fit_predict_evaluate, model_cv, TextSelector, NumberSelector, DenseTransformer, check_pos_tag, MeanEmbeddingVectorizer, TfidfEmbeddingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data source\n",
    "models = [\n",
    "            MultinomialNB(),\n",
    "            GaussianNB(),\n",
    "            BernoulliNB(),\n",
    "            LinearSVC(), \n",
    "            LogisticRegression(solver='liblinear', random_state=42, max_iter=100000),\n",
    "            DecisionTreeClassifier(),\n",
    "            KNeighborsClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "            xgboost.XGBClassifier(),\n",
    "            ExtraTreesClassifier(n_estimators=200),\n",
    "            VotingClassifier(estimators=[\n",
    "                ('lr', LogisticRegression(solver='liblinear', random_state=42, max_iter=100000)), \n",
    "                ('dt', DecisionTreeClassifier()), \n",
    "                ('lsvc', LinearSVC()), \n",
    "                ('knn', KNeighborsClassifier()), \n",
    "                ('mnb', MultinomialNB()), \n",
    "                ('nb', GaussianNB()), \n",
    "                ('bnb', BernoulliNB()), \n",
    "            ], voting='hard'),\n",
    "            AdaBoostClassifier(random_state=42), \n",
    "            GradientBoostingClassifier(learning_rate=0.01, random_state=42),\n",
    "        ]\n",
    "\n",
    "user_reviews = {}\n",
    "\n",
    "user_reviews['bug'] = {'data_train': 'Bug_Report_Data_Train.json', \n",
    "                      'not_data_train': 'Not_Bug_Report_Data_Train.json',\n",
    "                      'data_test': 'Bug_Report_Data_Test.json',\n",
    "                      'not_data_test': 'Not_Bug_Report_Data_Test.json',\n",
    "                      'label': 'Bug',\n",
    "                      'not_label': 'Not Bug'}\n",
    "\n",
    "user_reviews['feature'] = {'data_train': 'Feature_OR_Improvment_Request_Data_Train.json', \n",
    "                          'not_data_train': 'Not_Feature_OR_Improvment_Request_Data_Train.json',\n",
    "                          'data_test': 'Feature_OR_Improvment_Request_Data_Test.json',\n",
    "                          'not_data_test': 'Not_Feature_OR_Improvment_Request_Data_Test.json',\n",
    "                          'label': 'Feature',\n",
    "                          'not_label': 'Not Feature'}\n",
    "\n",
    "user_reviews['ux'] = {'data_train': 'UserExperience_Data_Train.json', \n",
    "                        'not_data_train': 'Not_UserExperience_Data_Train.json',\n",
    "                        'data_test': 'UserExperience_Data_Test.json',\n",
    "                        'not_data_test': 'Not_UserExperience_Data_Test.json',\n",
    "                        'label': 'UserExperience',\n",
    "                        'not_label': 'Not UserExperience'}\n",
    "\n",
    "user_reviews['rating'] = {'data_train': 'Rating_Data_Train.json', \n",
    "                          'not_data_train': 'Not_Rating_Data_Train.json',\n",
    "                          'data_test': 'Rating_Data_Test.json',\n",
    "                          'not_data_test': 'Not_Rating_Data_Test.json',\n",
    "                         'label': 'Rating',\n",
    "                         'not_label': 'Not Rating'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_review_type = user_reviews['bug'] # bug, feature, ux, rating\n",
    "pprint.pprint(selected_review_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('../RE2015_data/json_data/' + selected_review_type['data_train']) as data_file:    \n",
    "    data_train = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['not_data_train']) as data_file:    \n",
    "    not_data_train = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['data_test']) as data_file:    \n",
    "    data_test = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['not_data_test']) as data_file:    \n",
    "    not_data_test = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data frame\n",
    "data_train = pd.DataFrame.from_dict(data_train, orient='columns')\n",
    "data_train['label'] = selected_review_type['label']\n",
    "\n",
    "data_test = pd.DataFrame.from_dict(data_test, orient='columns')\n",
    "data_test['label'] = selected_review_type['label']\n",
    "\n",
    "not_data_train = pd.DataFrame.from_dict(not_data_train, orient='columns')\n",
    "not_data_train['label'] = selected_review_type['not_label']\n",
    "\n",
    "not_data_test = pd.DataFrame.from_dict(not_data_test, orient='columns')\n",
    "not_data_test['label'] = selected_review_type['not_label']\n",
    "\n",
    "df_train = data_train.append(not_data_train, ignore_index=True)\n",
    "df_test = data_test.append(not_data_test, ignore_index=True)\n",
    "\n",
    "df = df_train.append(df_test, ignore_index=True)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('label').count().plot.bar(ylim=0)\n",
    "plt.show()\n",
    "print(pd.value_counts(pd.Series(df['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_train:\\n', data_train.count(), '\\n')\n",
    "print('not_data_train:\\n', not_data_train.count(), '\\n')\n",
    "print('data_test:\\n', data_test.count(), '\\n')\n",
    "print('not_data_test:\\n', not_data_test.count(), '\\n')\n",
    "print('df_train:\\n', df_train.count(), '\\n')\n",
    "print('df_test:\\n', df_test.count(), '\\n')\n",
    "print('df:\\n', df.count(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature list\n",
    "f1 = ['comment']\n",
    "f2 = ['lemmatized_comment']\n",
    "f3 = ['stopwords_removal']\n",
    "f4 = ['stopwords_removal_lemmatization']\n",
    "f5 = ['rating', 'length_words']\n",
    "f6 = ['rating', 'length_words', 'present_simple', 'present_con', 'past', 'future']\n",
    "f7 = ['rating', 'length_words', 'present_simple', 'present_con', 'past', 'future', 'sentiScore']\n",
    "f8 = ['comment', 'rating', 'sentiScore']\n",
    "f9 = ['comment', 'rating', 'sentiScore', 'present_simple', 'present_con', 'past', 'future']\n",
    "f10 = ['stopwords_removal_lemmatization', 'rating', 'sentiScore', 'present_simple', 'present_con', 'past', 'future']\n",
    "\n",
    "f11 = ['comment', \n",
    "       'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f12 = ['lemmatized_comment', \n",
    "       'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f13 = ['stopwords_removal', \n",
    "       'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f14 = ['stopwords_removal_lemmatization', \n",
    "       'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f15 = ['comment', 'rating', 'sentiScore', \n",
    "       'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f16 = ['comment', 'rating', 'sentiScore', 'present_simple', 'present_con', 'past', 'future', \n",
    "       'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f17 = ['stopwords_removal_lemmatization', 'rating', 'sentiScore', 'present_simple', 'present_con', 'past', 'future', \n",
    "       'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f18 = ['rating', 'length_words', \n",
    "        'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f19 = ['rating', 'length_words', 'present_simple', 'present_con', 'past', 'future',\n",
    "        'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "f20 = ['rating', 'length_words', 'present_simple', 'present_con', 'past', 'future', 'sentiScore',\n",
    "       'noun', \n",
    "       'pron', \n",
    "       'verb', \n",
    "       'adj', \n",
    "       'adv',\n",
    "      ]\n",
    "\n",
    "features = [\n",
    "            f1, \n",
    "            f2,\n",
    "            f3,\n",
    "            f4,\n",
    "            f5,\n",
    "            f6,\n",
    "            f7,\n",
    "            f8,\n",
    "            f9,\n",
    "            f10,\n",
    "            f11,\n",
    "            f12,\n",
    "            f13,\n",
    "            f14,\n",
    "            f15,\n",
    "            f16,\n",
    "            f17,\n",
    "            f18, \n",
    "            f19, \n",
    "            f20,\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare list of feature union\n",
    "features_unions = []\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3, ngram_range=(1, 2))\n",
    "\n",
    "def compute_pos_text(data_frame, tag, text):\n",
    "    new_data_frame = data_frame\n",
    "    new_data_frame[tag] = new_data_frame[text].apply(lambda x: check_pos_tag(x, tag))\n",
    "    return new_data_frame\n",
    "\n",
    "for feature in features:\n",
    "    pipeline_tuples = []\n",
    "    \n",
    "    if ('comment' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', TextSelector(key='comment')),\n",
    "                ('tfidf', tfidf),\n",
    "                ('dense', DenseTransformer())\n",
    "            ])\n",
    "\n",
    "        pipeline_tuples.append(('comments', pip))\n",
    "        \n",
    "    if ('lemmatized_comment' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', TextSelector(key='lemmatized_comment')),\n",
    "                ('tfidf', tfidf),\n",
    "                ('dense', DenseTransformer())\n",
    "            ])\n",
    "        pipeline_tuples.append(('lemmatized_comments', pip))\n",
    "        \n",
    "    if ('stopwords_removal' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', TextSelector(key='stopwords_removal')),\n",
    "                ('tfidf', tfidf),\n",
    "                ('dense', DenseTransformer())\n",
    "            ])\n",
    "        pipeline_tuples.append(('stopwords_removals', pip))\n",
    "        \n",
    "    if ('stopwords_removal_lemmatization' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', TextSelector(key='stopwords_removal_lemmatization')),\n",
    "                ('tfidf', tfidf),\n",
    "                ('dense', DenseTransformer())\n",
    "            ])\n",
    "        pipeline_tuples.append(('stopwords_removal_lemmatizations', pip))\n",
    "        \n",
    "    if ('rating' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='rating')),\n",
    "                ('imp', SimpleImputer(missing_values=np.nan, strategy='mean'))\n",
    "            ])\n",
    "        pipeline_tuples.append(('ratings', pip))\n",
    "        \n",
    "    if ('present_simple' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='present_simple')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('present_simples', pip))\n",
    "        \n",
    "    if ('present_con' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='present_con')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('present_cons', pip))\n",
    "        \n",
    "    if ('past' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='past')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('pasts', pip))\n",
    "        \n",
    "    if ('future' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='future')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('futures', pip))\n",
    "        \n",
    "    if ('length_words' in feature):\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='length_words')),\n",
    "                ('imp', SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('length_words', pip))\n",
    "        \n",
    "    if ('sentiScore' in feature):\n",
    "        pip = Pipeline([\n",
    "                    ('selector', NumberSelector(key='sentiScore')),\n",
    "                    ('scaler', MinMaxScaler())\n",
    "                ])\n",
    "        pipeline_tuples.append(('sentiments', pip))\n",
    "        \n",
    "    # Part of Speech Tags (feature[0] must be text)\n",
    "    # Noun\n",
    "    if ('noun' in feature):\n",
    "        df_train = compute_pos_text(df_train, 'noun', feature[0])\n",
    "        df_test = compute_pos_text(df_test, 'noun', feature[0])\n",
    "        df = compute_pos_text(df, 'noun', feature[0])\n",
    "\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='noun')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('noun', pip))\n",
    "        \n",
    "    # Pronoun\n",
    "    if ('pron' in feature):\n",
    "        df_train = compute_pos_text(df_train, 'pron', feature[0])\n",
    "        df_test = compute_pos_text(df_test, 'pron', feature[0])\n",
    "        df = compute_pos_text(df, 'pron', feature[0])\n",
    "\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='pron')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('pron', pip))\n",
    "        \n",
    "    # Verb\n",
    "    if ('verb' in feature):\n",
    "        df_train = compute_pos_text(df_train, 'verb', feature[0])\n",
    "        df_test = compute_pos_text(df_test, 'verb', feature[0])\n",
    "        df = compute_pos_text(df, 'verb', feature[0])\n",
    "\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='verb')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('verb', pip))\n",
    "        \n",
    "    # Adjective\n",
    "    if ('adj' in feature):\n",
    "        df_train = compute_pos_text(df_train, 'adj', feature[0])\n",
    "        df_test = compute_pos_text(df_test, 'adj', feature[0])\n",
    "        df = compute_pos_text(df, 'adj', feature[0])\n",
    "\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='adj')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('adj', pip))\n",
    "        \n",
    "    # Adverb\n",
    "    if ('adv' in feature):\n",
    "        df_train = compute_pos_text(df_train, 'adv', feature[0])\n",
    "        df_test = compute_pos_text(df_test, 'adv', feature[0])\n",
    "        df = compute_pos_text(df, 'adv', feature[0])\n",
    "\n",
    "        pip = Pipeline([\n",
    "                ('selector', NumberSelector(key='adv')),\n",
    "                ('scaler1', StandardScaler()),\n",
    "                ('scaler2', MinMaxScaler())\n",
    "            ])\n",
    "        pipeline_tuples.append(('adv', pip))\n",
    "        \n",
    "    features_unions.append((FeatureUnion(pipeline_tuples), feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare targets\n",
    "y_train = df_train['label']\n",
    "y_test = df_test['label']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rank = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "for feats, feature_list in features_unions:\n",
    "    X_train = df_train[feature_list]\n",
    "    X_test = df_test[feature_list]\n",
    "    X = df[feature_list]\n",
    "    \n",
    "    for model in models:\n",
    "        pipeline = Pipeline([\n",
    "                            ('features', feats),\n",
    "                            ('classifier', model),\n",
    "                            ])\n",
    "        \n",
    "        desc = '%s/%s + %s' % (selected_review_type['label'], str(feature_list), model.__class__.__name__)\n",
    "        acc = fit_predict_evaluate(pipeline, X_train, X_test, y_train, y_test, desc)\n",
    "        accuracy_rank.append((acc, desc))\n",
    "        \n",
    "        desc_cv = '%s/%s + CV + %s' % (selected_review_type['label'], str(feature_list), model.__class__.__name__)\n",
    "        acc_cv = model_cv(5, X, y, pipeline, description=desc_cv, category=selected_review_type['label'])\n",
    "        accuracy_rank.append((acc_cv, desc_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write sorted results to text file.\n",
    "text_file = open('results/Rank/' + selected_review_type['label'] + '_common_rank_output.txt', 'w')\n",
    "sorted_accuracy_rank = sorted(accuracy_rank, key=lambda accuracy_rank: accuracy_rank[0], reverse=True)\n",
    "for item in sorted_accuracy_rank:\n",
    "    text = '%f , %s' % (item[0], item[1]) \n",
    "    print(text)\n",
    "    text_file.write(text + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
