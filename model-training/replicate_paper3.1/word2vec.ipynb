{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import pprint\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# import xgboost\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from ipynb.fs.defs.helper import report_to_csv, export_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data source\n",
    "models = [\n",
    "            GaussianNB(),\n",
    "            BernoulliNB(),\n",
    "            LinearSVC(), \n",
    "            LogisticRegression(solver='liblinear', random_state=42, max_iter=100000),\n",
    "            DecisionTreeClassifier(),\n",
    "            KNeighborsClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "            ExtraTreesClassifier(n_estimators=200),\n",
    "            VotingClassifier(estimators=[\n",
    "                ('lr', LogisticRegression(solver='liblinear', random_state=42, max_iter=100000)), \n",
    "                ('dt', DecisionTreeClassifier()), \n",
    "                ('lsvc', LinearSVC()), \n",
    "                ('knn', KNeighborsClassifier()),  \n",
    "                ('nb', GaussianNB()), \n",
    "                ('bnb', BernoulliNB()), \n",
    "            ], voting='hard'), \n",
    "            AdaBoostClassifier(random_state=42), \n",
    "            GradientBoostingClassifier(learning_rate=0.01, random_state=42),\n",
    "        ]\n",
    "\n",
    "user_reviews = {}\n",
    "\n",
    "user_reviews['bug'] = {'data_train': 'Bug_Report_Data_Train.json', \n",
    "                      'not_data_train': 'Not_Bug_Report_Data_Train.json',\n",
    "                      'data_test': 'Bug_Report_Data_Test.json',\n",
    "                      'not_data_test': 'Not_Bug_Report_Data_Test.json',\n",
    "                      'label': 'Bug',\n",
    "                      'not_label': 'Not Bug'}\n",
    "\n",
    "user_reviews['feature'] = {'data_train': 'Feature_OR_Improvment_Request_Data_Train.json', \n",
    "                          'not_data_train': 'Not_Feature_OR_Improvment_Request_Data_Train.json',\n",
    "                          'data_test': 'Feature_OR_Improvment_Request_Data_Test.json',\n",
    "                          'not_data_test': 'Not_Feature_OR_Improvment_Request_Data_Test.json',\n",
    "                          'label': 'Feature',\n",
    "                          'not_label': 'Not Feature'}\n",
    "\n",
    "user_reviews['ux'] = {'data_train': 'UserExperience_Data_Train.json', \n",
    "                        'not_data_train': 'Not_UserExperience_Data_Train.json',\n",
    "                        'data_test': 'UserExperience_Data_Test.json',\n",
    "                        'not_data_test': 'Not_UserExperience_Data_Test.json',\n",
    "                        'label': 'UserExperience',\n",
    "                        'not_label': 'Not UserExperience'}\n",
    "\n",
    "user_reviews['rating'] = {'data_train': 'Rating_Data_Train.json', \n",
    "                          'not_data_train': 'Not_Rating_Data_Train.json',\n",
    "                          'data_test': 'Rating_Data_Test.json',\n",
    "                          'not_data_test': 'Not_Rating_Data_Test.json',\n",
    "                         'label': 'Rating',\n",
    "                         'not_label': 'Not Rating'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_review_type = user_reviews['bug'] # bug, feature, ux, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open('../RE2015_data/json_data/' + selected_review_type['data_train']) as data_file:    \n",
    "    data_train = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['not_data_train']) as data_file:    \n",
    "    not_data_train = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['data_test']) as data_file:    \n",
    "    data_test = json.load(data_file)\n",
    "    \n",
    "with open('../RE2015_data/json_data/' + selected_review_type['not_data_test']) as data_file:    \n",
    "    not_data_test = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data frame\n",
    "data_train = pd.DataFrame.from_dict(data_train, orient='columns')\n",
    "data_train['label'] = selected_review_type['label']\n",
    "\n",
    "data_test = pd.DataFrame.from_dict(data_test, orient='columns')\n",
    "data_test['label'] = selected_review_type['label']\n",
    "\n",
    "not_data_train = pd.DataFrame.from_dict(not_data_train, orient='columns')\n",
    "not_data_train['label'] = selected_review_type['not_label']\n",
    "\n",
    "not_data_test = pd.DataFrame.from_dict(not_data_test, orient='columns')\n",
    "not_data_test['label'] = selected_review_type['not_label']\n",
    "\n",
    "df_train = data_train.append(not_data_train, ignore_index=True)\n",
    "df_test = data_test.append(not_data_test, ignore_index=True)\n",
    "\n",
    "df = df_train.append(df_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment1 = \"This app serves its purpose for me perfectly except for the mobile deposit won't work. It keeps saying can't find endorsement. After calling PNC multiple times about this still no fix.\"\n",
    "# stopwords_removal1 = \"this app serves purpose for perfectly except for mobile deposit wont work keeps saying cant find endorsement after calling pnc multiple times about this still no fix\"\n",
    "# lemmatized_comment1 = \"this app serve it purpose for me perfectly except for the mobile deposit wont work it keep say cant find endorsement after call pnc multiple time about this still no fix\"\n",
    "# stopwords_removal_lemmatization1 = \"this app serve purpose for perfectly except for mobile deposit wont work keep say cant find endorsement after call pnc multiple time about this still no fix\"\n",
    "\n",
    "# word2vec_vector = word_averaging_list(wv, w2v_tokenize_text(stopwords_removal_lemmatization1))\n",
    "# print(word2vec_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "text_features = [\n",
    "                'comment', \n",
    "                'lemmatized_comment', \n",
    "                'stopwords_removal',\n",
    "                'stopwords_removal_lemmatization',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rank = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for text in text_features:\n",
    "    test_tokenized = test.apply(lambda r: w2v_tokenize_text(r[text]), axis=1).values\n",
    "    train_tokenized = train.apply(lambda r: w2v_tokenize_text(r[text]), axis=1).values\n",
    "    X_train_word_average = word_averaging_list(wv, train_tokenized)\n",
    "    X_test_word_average = word_averaging_list(wv, test_tokenized)\n",
    "    \n",
    "    for model in models:\n",
    "        print(model.__class__.__name__)\n",
    "        print(text)\n",
    "        \n",
    "        model.fit(X_train_word_average, train['label'])\n",
    "        y_pred = model.predict(X_test_word_average)\n",
    "        \n",
    "        accuracy = accuracy_score(test.label, y_pred)\n",
    "        print('accuracy %s\\n' % accuracy)\n",
    "        \n",
    "        description = '%s/word2vec + %s + %s' % (selected_review_type['label'], text, model.__class__.__name__)\n",
    "        accuracy_rank.append((accuracy, description))\n",
    "        \n",
    "        # Export model\n",
    "        export_model(model, file_name=description)\n",
    "        \n",
    "        report = classification_report(test.label, y_pred, output_dict=True) \n",
    "        report['accuracy'] = {' ': accuracy}\n",
    "        report_to_csv(report, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Word2Vec with cross validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = df[text_features]\n",
    "y = df[['label']]\n",
    "\n",
    "for model in models:\n",
    "    for text in text_features:\n",
    "        accuracy = []\n",
    "\n",
    "        for train, test in kfold.split(X, y):    \n",
    "            test_tokenized = X.iloc[test].apply(lambda r: w2v_tokenize_text(r[text]), axis=1).values\n",
    "            train_tokenized = X.iloc[train].apply(lambda r: w2v_tokenize_text(r[text]), axis=1).values\n",
    "            X_train_word_average = word_averaging_list(wv, train_tokenized)\n",
    "            X_test_word_average = word_averaging_list(wv, test_tokenized)\n",
    "\n",
    "            model.fit(X_train_word_average, y.iloc[train])\n",
    "            prediction = model.predict(X_test_word_average)\n",
    "            accuracy.append(accuracy_score(y.iloc[test], prediction))\n",
    "\n",
    "        avg_acc = np.mean(accuracy)\n",
    "        description = '%s/word2vec + CV + %s + %s' % (selected_review_type['label'], text, model.__class__.__name__)\n",
    "        accuracy_rank.append((avg_acc, description))\n",
    "        \n",
    "        print(model.__class__.__name__)\n",
    "        print(text)\n",
    "        print(avg_acc)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sorted results to text file.\n",
    "text_file = open('results/Rank/' + selected_review_type['label'] + '_word2vec_rank_output.txt', 'w')\n",
    "sorted_accuracy_rank = sorted(accuracy_rank, key=lambda accuracy_rank: accuracy_rank[0], reverse=True)\n",
    "for item in sorted_accuracy_rank:\n",
    "    text = '%f , %s' % (item[0], item[1]) \n",
    "    print(text)\n",
    "    text_file.write(text + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
