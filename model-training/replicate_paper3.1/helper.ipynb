{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import textblob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model, file_name):\n",
    "    #serializing our model to a file called {file_name}.pkl\n",
    "    pickle.dump(model, open('models/' + file_name + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_to_csv(report_dict, output_file_name):\n",
    "    df = pd.DataFrame(report_dict).transpose()\n",
    "    df.to_csv('results/' + output_file_name + '.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_evaluate(pipeline, X_train, X_test, y_train, y_test, description):\n",
    "    print(description)\n",
    "    \n",
    "    # Fitting\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Export model\n",
    "    export_model(pipeline, file_name=description)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy)\n",
    "    print()\n",
    "    \n",
    "    # Export to .csv\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report['accuracy'] = {' ': accuracy}\n",
    "    report_to_csv(report, description)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cv(splits, X, y, pipeline, description, category):\n",
    "    print(description)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "    reports = []\n",
    "    accuracy = []\n",
    "    \n",
    "    for train, test in kfold.split(X, y):\n",
    "        model_fit = pipeline.fit(X.iloc[train], y.iloc[train])\n",
    "        prediction = model_fit.predict(X.iloc[test])\n",
    "        accuracy.append(accuracy_score(y.iloc[test], prediction))\n",
    "        reports.append(classification_report(y.iloc[test], prediction, output_dict=True))\n",
    "    \n",
    "    if (category == 'Bug'):\n",
    "        report_avg_dict = bug_report_avg(reports)\n",
    "    elif (category == 'Feature'):\n",
    "        report_avg_dict = feature_report_avg(reports)\n",
    "    elif (category == 'UserExperience'):\n",
    "        report_avg_dict = ux_report_avg(reports)\n",
    "    elif (category == 'Rating'):\n",
    "        report_avg_dict = rating_report_avg(reports)\n",
    "    else:\n",
    "        raise ValueError('Category must be Bug, Feature, UX, or Rating.')\n",
    "    \n",
    "    accuracy = np.mean(accuracy)\n",
    "    print('Accuracy:', accuracy)\n",
    "    print()\n",
    "    \n",
    "    # Export to .csv\n",
    "    report_avg_dict['accuracy'] = {' ': accuracy}\n",
    "    report_to_csv(report_avg_dict, description)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bug_report_avg(reports):\n",
    "    df = pd.DataFrame.from_dict(reports)\n",
    "    \n",
    "    bug_arr = df['Bug'].values \n",
    "    not_bug_arr = df['Not Bug'].values\n",
    "\n",
    "    macro_arr = df['macro avg'].values \n",
    "    micro_arr = df['micro avg'].values \n",
    "    weighted_arr = df['weighted avg'].values\n",
    "    \n",
    "    # lambda functions\n",
    "    precision_lambda = lambda x: x['precision']\n",
    "    recall_lambda = lambda x: x['recall']\n",
    "    f1_lambda = lambda x: x['f1-score']\n",
    "    support_lambda = lambda x: x['support']\n",
    "    \n",
    "    # Mean function\n",
    "    def my_mean(function, array):\n",
    "        return np.mean(np.array(list(map(function, array))))\n",
    "    \n",
    "    # Bug\n",
    "    bugs_precision_avg = my_mean(precision_lambda, bug_arr)\n",
    "    bugs_recall_avg = my_mean(recall_lambda, bug_arr)\n",
    "    bugs_f1_avg = my_mean(f1_lambda, bug_arr)\n",
    "    bugs_support_avg = my_mean(support_lambda, bug_arr)\n",
    "    \n",
    "    # Not Bug\n",
    "    not_bugs_precision_avg = my_mean(precision_lambda, not_bug_arr)\n",
    "    not_bugs_recall_avg = my_mean(recall_lambda, not_bug_arr)\n",
    "    not_bugs_f1_avg = my_mean(f1_lambda, not_bug_arr)\n",
    "    not_bugs_support_avg = my_mean(support_lambda, not_bug_arr)\n",
    "    \n",
    "    # Macro\n",
    "    macro_precision_avg = my_mean(precision_lambda, macro_arr)\n",
    "    macro_recall_avg = my_mean(recall_lambda, macro_arr)\n",
    "    macro_f1_avg = my_mean(f1_lambda, macro_arr)\n",
    "    macro_support_avg = my_mean(support_lambda, macro_arr)\n",
    "    \n",
    "    # Micro\n",
    "    micro_precision_avg = my_mean(precision_lambda, micro_arr)\n",
    "    micro_recall_avg = my_mean(recall_lambda, micro_arr)\n",
    "    micro_f1_avg = my_mean(f1_lambda, micro_arr)\n",
    "    micro_support_avg = my_mean(support_lambda, micro_arr)\n",
    "    \n",
    "    # Weight\n",
    "    weights_precision_avg = my_mean(precision_lambda, weighted_arr)\n",
    "    weights_recall_avg = my_mean(recall_lambda, weighted_arr)\n",
    "    weights_f1_avg = my_mean(f1_lambda, weighted_arr)\n",
    "    weights_support_avg = my_mean(support_lambda, weighted_arr)\n",
    "    \n",
    "    # Aggregate function\n",
    "    def my_dict(precision_avg, recall_avg, f1_avg, support_avg):\n",
    "        return {\"precision\": precision_avg, \n",
    "               \"recall\": recall_avg, \n",
    "               \"f1-score\": f1_avg, \n",
    "               \"support\": support_avg}\n",
    "    \n",
    "    # Combine data then return\n",
    "    return {\"Bug\": my_dict(bugs_precision_avg, bugs_recall_avg, bugs_f1_avg, bugs_support_avg),\n",
    "            \"Not Bug\": my_dict(not_bugs_precision_avg, not_bugs_recall_avg, not_bugs_f1_avg, not_bugs_support_avg), \n",
    "            \"micro avg\": my_dict(micro_precision_avg, micro_recall_avg, micro_f1_avg, micro_support_avg), \n",
    "            \"macro avg\": my_dict(macro_precision_avg, macro_recall_avg, macro_f1_avg, macro_support_avg), \n",
    "            \"weighted avg\": my_dict(weights_precision_avg, weights_recall_avg, weights_f1_avg, weights_support_avg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_report_avg(reports):\n",
    "    df = pd.DataFrame.from_dict(reports)\n",
    "    \n",
    "    feature_arr = df['Feature'].values\n",
    "    not_feature_arr = df['Not Feature'].values\n",
    "\n",
    "    macro_arr = df['macro avg'].values \n",
    "    micro_arr = df['micro avg'].values \n",
    "    weighted_arr = df['weighted avg'].values\n",
    "    \n",
    "    # lambda functions\n",
    "    precision_lambda = lambda x: x['precision']\n",
    "    recall_lambda = lambda x: x['recall']\n",
    "    f1_lambda = lambda x: x['f1-score']\n",
    "    support_lambda = lambda x: x['support']\n",
    "    \n",
    "    # Mean function\n",
    "    def my_mean(function, array):\n",
    "        return np.mean(np.array(list(map(function, array))))\n",
    "    \n",
    "    # Feature\n",
    "    features_precision_avg = my_mean(precision_lambda, feature_arr)\n",
    "    features_recall_avg = my_mean(recall_lambda, feature_arr)\n",
    "    features_f1_avg = my_mean(f1_lambda, feature_arr)\n",
    "    features_support_avg = my_mean(support_lambda, feature_arr)\n",
    "    \n",
    "    # Not Feature\n",
    "    not_features_precision_avg = my_mean(precision_lambda, not_feature_arr)\n",
    "    not_features_recall_avg = my_mean(recall_lambda, not_feature_arr)\n",
    "    not_features_f1_avg = my_mean(f1_lambda, not_feature_arr)\n",
    "    not_features_support_avg = my_mean(support_lambda, not_feature_arr)\n",
    "    \n",
    "    # Macro\n",
    "    macro_precision_avg = my_mean(precision_lambda, macro_arr)\n",
    "    macro_recall_avg = my_mean(recall_lambda, macro_arr)\n",
    "    macro_f1_avg = my_mean(f1_lambda, macro_arr)\n",
    "    macro_support_avg = my_mean(support_lambda, macro_arr)\n",
    "    \n",
    "    # Micro\n",
    "    micro_precision_avg = my_mean(precision_lambda, micro_arr)\n",
    "    micro_recall_avg = my_mean(recall_lambda, micro_arr)\n",
    "    micro_f1_avg = my_mean(f1_lambda, micro_arr)\n",
    "    micro_support_avg = my_mean(support_lambda, micro_arr)\n",
    "    \n",
    "    # Weight\n",
    "    weights_precision_avg = my_mean(precision_lambda, weighted_arr)\n",
    "    weights_recall_avg = my_mean(recall_lambda, weighted_arr)\n",
    "    weights_f1_avg = my_mean(f1_lambda, weighted_arr)\n",
    "    weights_support_avg = my_mean(support_lambda, weighted_arr)\n",
    "    \n",
    "    # Aggregate function\n",
    "    def my_dict(precision_avg, recall_avg, f1_avg, support_avg):\n",
    "        return {\"precision\": precision_avg, \n",
    "               \"recall\": recall_avg, \n",
    "               \"f1-score\": f1_avg, \n",
    "               \"support\": support_avg}\n",
    "    \n",
    "    # Combine data then return\n",
    "    return {\"Feature\": my_dict(features_precision_avg, features_recall_avg, features_f1_avg, features_support_avg),\n",
    "            \"Not Feature\": my_dict(not_features_precision_avg, not_features_recall_avg, not_features_f1_avg, not_features_support_avg),\n",
    "            \"micro avg\": my_dict(micro_precision_avg, micro_recall_avg, micro_f1_avg, micro_support_avg), \n",
    "            \"macro avg\": my_dict(macro_precision_avg, macro_recall_avg, macro_f1_avg, macro_support_avg), \n",
    "            \"weighted avg\": my_dict(weights_precision_avg, weights_recall_avg, weights_f1_avg, weights_support_avg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ux_report_avg(reports):\n",
    "    df = pd.DataFrame.from_dict(reports)\n",
    "    \n",
    "    ux_arr = df['UserExperience'].values\n",
    "    not_ux_arr = df['Not UserExperience'].values\n",
    "\n",
    "    macro_arr = df['macro avg'].values \n",
    "    micro_arr = df['micro avg'].values \n",
    "    weighted_arr = df['weighted avg'].values\n",
    "    \n",
    "    # lambda functions\n",
    "    precision_lambda = lambda x: x['precision']\n",
    "    recall_lambda = lambda x: x['recall']\n",
    "    f1_lambda = lambda x: x['f1-score']\n",
    "    support_lambda = lambda x: x['support']\n",
    "    \n",
    "    # Mean function\n",
    "    def my_mean(function, array):\n",
    "        return np.mean(np.array(list(map(function, array))))\n",
    "    \n",
    "    # UX\n",
    "    ux_precision_avg = my_mean(precision_lambda, ux_arr)\n",
    "    ux_recall_avg = my_mean(recall_lambda, ux_arr)\n",
    "    ux_f1_avg = my_mean(f1_lambda, ux_arr)\n",
    "    ux_support_avg = my_mean(support_lambda, ux_arr)\n",
    "    \n",
    "    # Not UX\n",
    "    not_ux_precision_avg = my_mean(precision_lambda, not_ux_arr)\n",
    "    not_ux_recall_avg = my_mean(recall_lambda, not_ux_arr)\n",
    "    not_ux_f1_avg = my_mean(f1_lambda, not_ux_arr)\n",
    "    not_ux_support_avg = my_mean(support_lambda, not_ux_arr)\n",
    "    \n",
    "    # Macro\n",
    "    macro_precision_avg = my_mean(precision_lambda, macro_arr)\n",
    "    macro_recall_avg = my_mean(recall_lambda, macro_arr)\n",
    "    macro_f1_avg = my_mean(f1_lambda, macro_arr)\n",
    "    macro_support_avg = my_mean(support_lambda, macro_arr)\n",
    "    \n",
    "    # Micro\n",
    "    micro_precision_avg = my_mean(precision_lambda, micro_arr)\n",
    "    micro_recall_avg = my_mean(recall_lambda, micro_arr)\n",
    "    micro_f1_avg = my_mean(f1_lambda, micro_arr)\n",
    "    micro_support_avg = my_mean(support_lambda, micro_arr)\n",
    "    \n",
    "    # Weight\n",
    "    weights_precision_avg = my_mean(precision_lambda, weighted_arr)\n",
    "    weights_recall_avg = my_mean(recall_lambda, weighted_arr)\n",
    "    weights_f1_avg = my_mean(f1_lambda, weighted_arr)\n",
    "    weights_support_avg = my_mean(support_lambda, weighted_arr)\n",
    "    \n",
    "    # Aggregate function\n",
    "    def my_dict(precision_avg, recall_avg, f1_avg, support_avg):\n",
    "        return {\"precision\": precision_avg, \n",
    "               \"recall\": recall_avg, \n",
    "               \"f1-score\": f1_avg, \n",
    "               \"support\": support_avg}\n",
    "    \n",
    "    # Combine data then return\n",
    "    return {\"UserExperience\": my_dict(ux_precision_avg, ux_recall_avg, ux_f1_avg, ux_support_avg),\n",
    "            \"Not UserExperience\": my_dict(not_ux_precision_avg, not_ux_recall_avg, not_ux_f1_avg, not_ux_support_avg),\n",
    "            \"micro avg\": my_dict(micro_precision_avg, micro_recall_avg, micro_f1_avg, micro_support_avg), \n",
    "            \"macro avg\": my_dict(macro_precision_avg, macro_recall_avg, macro_f1_avg, macro_support_avg), \n",
    "            \"weighted avg\": my_dict(weights_precision_avg, weights_recall_avg, weights_f1_avg, weights_support_avg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_report_avg(reports):\n",
    "    df = pd.DataFrame.from_dict(reports)\n",
    "     \n",
    "    rating_arr = df['Rating'].values\n",
    "    not_rating_arr = df['Not Rating'].values\n",
    "\n",
    "    macro_arr = df['macro avg'].values \n",
    "    micro_arr = df['micro avg'].values \n",
    "    weighted_arr = df['weighted avg'].values\n",
    "    \n",
    "    # lambda functions\n",
    "    precision_lambda = lambda x: x['precision']\n",
    "    recall_lambda = lambda x: x['recall']\n",
    "    f1_lambda = lambda x: x['f1-score']\n",
    "    support_lambda = lambda x: x['support']\n",
    "    \n",
    "    # Mean function\n",
    "    def my_mean(function, array):\n",
    "        return np.mean(np.array(list(map(function, array))))\n",
    "    \n",
    "    # Rating\n",
    "    ratings_precision_avg = my_mean(precision_lambda, rating_arr)\n",
    "    ratings_recall_avg = my_mean(recall_lambda, rating_arr)\n",
    "    ratings_f1_avg = my_mean(f1_lambda, rating_arr)\n",
    "    ratings_support_avg = my_mean(support_lambda, rating_arr)\n",
    "    \n",
    "    # Not Rating\n",
    "    not_ratings_precision_avg = my_mean(precision_lambda, not_rating_arr)\n",
    "    not_ratings_recall_avg = my_mean(recall_lambda, not_rating_arr)\n",
    "    not_ratings_f1_avg = my_mean(f1_lambda, not_rating_arr)\n",
    "    not_ratings_support_avg = my_mean(support_lambda, not_rating_arr)\n",
    "    \n",
    "    # Macro\n",
    "    macro_precision_avg = my_mean(precision_lambda, macro_arr)\n",
    "    macro_recall_avg = my_mean(recall_lambda, macro_arr)\n",
    "    macro_f1_avg = my_mean(f1_lambda, macro_arr)\n",
    "    macro_support_avg = my_mean(support_lambda, macro_arr)\n",
    "    \n",
    "    # Micro\n",
    "    micro_precision_avg = my_mean(precision_lambda, micro_arr)\n",
    "    micro_recall_avg = my_mean(recall_lambda, micro_arr)\n",
    "    micro_f1_avg = my_mean(f1_lambda, micro_arr)\n",
    "    micro_support_avg = my_mean(support_lambda, micro_arr)\n",
    "    \n",
    "    # Weight\n",
    "    weights_precision_avg = my_mean(precision_lambda, weighted_arr)\n",
    "    weights_recall_avg = my_mean(recall_lambda, weighted_arr)\n",
    "    weights_f1_avg = my_mean(f1_lambda, weighted_arr)\n",
    "    weights_support_avg = my_mean(support_lambda, weighted_arr)\n",
    "    \n",
    "    # Aggregate function\n",
    "    def my_dict(precision_avg, recall_avg, f1_avg, support_avg):\n",
    "        return {\"precision\": precision_avg, \n",
    "               \"recall\": recall_avg, \n",
    "               \"f1-score\": f1_avg, \n",
    "               \"support\": support_avg}\n",
    "    \n",
    "    # Combine data then return\n",
    "    return {\"Rating\": my_dict(ratings_precision_avg, ratings_recall_avg, ratings_f1_avg, ratings_support_avg),\n",
    "            \"Not Rating\": my_dict(not_ratings_precision_avg, not_ratings_recall_avg, not_ratings_f1_avg, not_ratings_support_avg),\n",
    "            \"micro avg\": my_dict(micro_precision_avg, micro_recall_avg, micro_f1_avg, micro_support_avg), \n",
    "            \"macro avg\": my_dict(macro_precision_avg, macro_recall_avg, macro_f1_avg, macro_support_avg), \n",
    "            \"weighted avg\": my_dict(weights_precision_avg, weights_recall_avg, weights_f1_avg, weights_support_avg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    pos_family = {\n",
    "                    'noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "                    'pron': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "                    'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "                    'adj':  ['JJ', 'JJR', 'JJS'],\n",
    "                    'adv': ['RB', 'RBR', 'RBS', 'WRB']\n",
    "                }\n",
    "    \n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, dim):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec) > 0:\n",
    "            self.dim = dim\n",
    "        else:\n",
    "            self.dim = 0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, dim):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec) > 0:\n",
    "            self.dim = dim\n",
    "        else:\n",
    "            self.dim = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
